{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein embeddings improve phage-host interaction prediction\n",
    "\n",
    "**Mark Edward M. Gonzales<sup>1, 2</sup>, Jennifer C. Ureta<sup>1, 2</sup> & Anish M.S. Shrestha<sup>1, 2</sup>**\n",
    "\n",
    "<sup>1</sup> Bioinformatics Laboratory, Advanced Research Institute for Informatics, Computing and Networking, De La Salle University, Manila, Philippines <br>\n",
    "<sup>2</sup> Department of Software Technology, College of Computer Studies, De La Salle University, Manila, Philippines \n",
    "\n",
    "{mark_gonzales, jennifer.ureta, anish.shrestha}@dlsu.edu.ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Memory Requirement of Protein Embeddings\n",
    "\n",
    "The memory requirement of loading pretrained protein embeddings may be heavy for some local machines. We recommend running this notebook on [Google Colab](https://colab.research.google.com/) or any cloud-based service with GPU. In particular, the largest model, ProtT5, consumes 5.9 GB of GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° FASTA Files\n",
    "This notebook assumes that you have generated the FASTA files containing the annotated RBP and hypothetical protein sequences (from running [`1. Sequence Preprocessing.ipynb`](https://github.com/bioinfodlsu/phage-host-prediction/blob/main/experiments/1.%20Sequence%20Preprocessing.ipynb)). \n",
    "\n",
    "Alternatively, you may download the FASTA files from [Google Drive](https://drive.google.com/drive/folders/16ZBXZCpC0OmldtPPIy5sEBtS4EVohorT?usp=sharing). Save the downloaded `fasta` folder inside the `inphared` directory located in the same folder as this notebook. The folder structure should look like this:\n",
    "\n",
    "`experiments` (parent folder of this notebook) <br> \n",
    "‚Ü≥ `inphared` <br>\n",
    "&nbsp; &nbsp;‚Ü≥ `fasta` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `hypothetical` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `nucleotide` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `rbp` <br>\n",
    "‚Ü≥ `4. Protein Embedding Generation.ipynb` (this notebook) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Output Files\n",
    "If you would like to skip running this notebook, you may download the protein embeddings from these Google Drive directories: [Part 1](https://drive.google.com/drive/folders/1deenrDQIr3xcl9QCYH-nPhmpY8x2drQw?usp=sharing) and [Part 2](https://drive.google.com/drive/folders/1jnBFNsC6zJISkc6IAz56257MSXKjY0Ez?usp=sharing). Consolidate the downloaded folders into a single `embeddings` directory and save it inside the `inphared` directory located in the same folder as this notebook. The folder structure should look like this:\n",
    "\n",
    "`experiments` (parent folder of this notebook) <br> \n",
    "‚Ü≥ `inphared` <br>\n",
    "&nbsp; &nbsp;‚Ü≥ `embeddings` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `esm` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `esm1b` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ ... <br>\n",
    "‚Ü≥ `4. Protein Embedding Generation.ipynb` (this notebook) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLgiuc5oZcNu",
    "outputId": "37b73e1b-8b7b-4b5b-8747-1c230412a70d"
   },
   "outputs": [],
   "source": [
    "!pip3 install -U pip > /dev/null\n",
    "!pip3 install -U bio_embeddings[all] > /dev/null\n",
    "!pip install scikit_learn==1.0.2\n",
    "!pip install pyyaml==5.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi4yylSXZf6O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the protein language model. \n",
    "\n",
    "Protein Language Model | Import\n",
    "-- | --\n",
    "SeqVec | `SeqVecEmbedder`\n",
    "ESM | `ESMEmbedder`\n",
    "ESM-1b | `ESM1bEmbedder`\n",
    "ProtBert | `ProtTransBertBFDEmbedder`\n",
    "ProtXLNet | `ProtTransXLNetUniRef100Embedder`\n",
    "ProtAlbert | `ProtTransAlbertBFDEmbedder`\n",
    "ProtT5 | `ProtTransT5XLU50Embedder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTCyLpEbBxWJ",
    "outputId": "58450092-b4bd-4639-f7ea-4d46dc8533dd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkM73XzOB7MP"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Generation of Protein Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below generate the protein embeddings for the proteins in a given FASTA file:\n",
    "- Use `compute_protein_embeddings_esm` for ESM and ESM-1b. Sequences longer than 1022 amino acids are split into non-overlapping subsequences of length 1022, and the per-residue embeddings are concatenated before averaging (this is the [workaround](https://github.com/brianhie/evolocity/issues/2) suggested by the developers).\n",
    "- Use `compute_protein_embeddings` for all other language models.\n",
    "\n",
    "**Parameters**:\n",
    "- `embedder`: Protein language model\n",
    "- `fasta_file`: FASTA file containing the proteins\n",
    "- `results_dir`: File path of the directory to which the resulting embeddings will be saved\n",
    "- `prefix`: Name of the phage whose selected proteins are to be converted to be embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S43Mg3StZh4N"
   },
   "outputs": [],
   "source": [
    "def compute_protein_embeddings(embedder, fasta_file, results_dir, prefix=''):\n",
    "    names = [record.id for record in SeqIO.parse(fasta_file, 'fasta')]\n",
    "    sequences = [str(record.seq) for record in SeqIO.parse(fasta_file, 'fasta')]\n",
    "\n",
    "    embeddings = [embedder.reduce_per_protein(embedder.embed(sequence)) for sequence in tqdm(sequences)]\n",
    "    embeddings_df = pd.concat([pd.DataFrame({'ID': names}), pd.DataFrame(embeddings)], axis=1)\n",
    "    embeddings_df.to_csv(results_dir + prefix + '-embeddings.csv', index=False)\n",
    "\n",
    "\n",
    "def compute_protein_embeddings_esm(embedder, fasta_file, results_dir, prefix=''):\n",
    "    names = [record.id for record in SeqIO.parse(fasta_file, 'fasta')]\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for record in SeqIO.parse(fasta_file, 'fasta'):\n",
    "        sequence = str(record.seq)\n",
    "        if len(sequence) <= 1022:\n",
    "            embedding = embedder.reduce_per_protein(embedder.embed(sequence))\n",
    "        else:\n",
    "            embedding1 = embedder.embed(sequence[:1022])\n",
    "            embedding2 = embedder.embed(sequence[1022:])\n",
    "            embedding = embedder.reduce_per_protein(np.concatenate((embedding1, embedding2)))\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    embeddings_df = pd.concat([pd.DataFrame({'ID': names}), pd.DataFrame(embeddings)], axis=1)\n",
    "    embeddings_df.to_csv(results_dir + prefix + '-embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the protein language model.\n",
    "\n",
    "Protein Language Model | Constructor\n",
    "-- | --\n",
    "SeqVec | `SeqVecEmbedder`\n",
    "ESM | `ESMEmbedder`\n",
    "ESM-1b | `ESM1bEmbedder`\n",
    "ProtBert | `ProtTransBertBFDEmbedder`\n",
    "ProtXLNet | `ProtTransXLNetUniRef100Embedder`\n",
    "ProtAlbert | `ProtTransAlbertBFDEmbedder`\n",
    "ProtT5 | `ProtTransT5XLU50Embedder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfPGFC17bedX"
   },
   "outputs": [],
   "source": [
    "embedder = ProtTransBertBFDEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply the directory names:\n",
    "- `HYPOTHETICAL_FASTA_DIR`: Directory where the FASTA files containing the protein sequences are located\n",
    "- `HYPOTHETICAL_EMBEDDINGS_DIR`: Directory where the CSV files containing the embeddings are to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWdkkpilF-gF"
   },
   "outputs": [],
   "source": [
    "HYPOTHETICAL_FASTA_DIR = f''\n",
    "HYPOTHETICAL_EMBEDDINGS_DIR = f''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the FASTA files containing the protein sequences to be embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLNKoLIgF1l2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "hypothetical_fasta_files = os.listdir(HYPOTHETICAL_FASTA_DIR)\n",
    "\n",
    "len(hypothetical_fasta_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the protein embeddings.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: If the embedder is ESM or ESM-1b, call `compute_protein_embeddings_esm` instead of `compute_protein_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNbJ4LawFg9_"
   },
   "outputs": [],
   "source": [
    "IDX_RESUME = 0    # Adjust as needed (e.g., resuming after Google Colab hangs or times out)\n",
    "\n",
    "for hypothetical_file in hypothetical_fasta_files[IDX_RESUME:]:\n",
    "  # -6 because the string \".fasta\" has six characters\n",
    "  compute_protein_embeddings(embedder, f'{HYPOTHETICAL_FASTA_DIR}/{hypothetical_file}', \n",
    "                             HYPOTHETICAL_EMBEDDINGS_DIR,\n",
    "                             f'/{hypothetical_file[:-6]}')\n",
    "  \n",
    "  # Display progress\n",
    "  print(IDX_RESUME, \":\", hypothetical_file)\n",
    "  IDX_RESUME += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

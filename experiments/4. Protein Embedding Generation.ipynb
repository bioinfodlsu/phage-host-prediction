{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein embeddings improve phage-host interaction prediction\n",
    "\n",
    "**Mark Edward M. Gonzales<sup>1, 2</sup>, Jennifer C. Ureta<sup>1, 2</sup> & Anish M.S. Shrestha<sup>1, 2</sup>**\n",
    "\n",
    "<sup>1</sup> Bioinformatics Laboratory, Advanced Research Institute for Informatics, Computing and Networking, De La Salle University, Manila, Philippines <br>\n",
    "<sup>2</sup> Department of Software Technology, College of Computer Studies, De La Salle University, Manila, Philippines \n",
    "\n",
    "{mark_gonzales, jennifer.ureta, anish.shrestha}@dlsu.edu.ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Memory Requirement of Protein Embeddings\n",
    "\n",
    "The memory requirement of loading pretrained protein language models may be heavy for some local machines. In particular, the largest model, ProtT5, consumes 5.9 GB of GPU memory. If your local machine does not have a GPU or if its GPU has insufficient memory to load ProtT5, we recommend running this notebook on a cloud-based service with GPU. \n",
    "\n",
    "**‚ö†Ô∏è UPDATE (06/12/2023):** In May 2023, Google Colab switched its default runtime to Python 3.10. However, one of this notebook's dependencies, [`bio-embeddings`](https://docs.bioembeddings.com/v0.2.3/) (v0.2.3), seems to be incompatible with Python 3.10.\n",
    "\n",
    "An alternative cloud-based service with GPU is Paperspace; you may try using either its [PyTorch 1.12 runtime](https://docs.paperspace.com/gradient/notebooks/runtimes/#recommended-runtimes) (which, as of writing, uses Python 3.9) or [Python 3.9 runtime](https://docs.paperspace.com/gradient/notebooks/runtimes/#previous-runtime-versions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° FASTA Files\n",
    "This notebook assumes that you have generated the FASTA files containing the annotated RBP and hypothetical protein sequences (from running [`1. Sequence Preprocessing.ipynb`](https://github.com/bioinfodlsu/phage-host-prediction/blob/main/experiments/1.%20Sequence%20Preprocessing.ipynb)). \n",
    "\n",
    "Alternatively, you may download the FASTA files from [Google Drive](https://drive.google.com/drive/folders/16ZBXZCpC0OmldtPPIy5sEBtS4EVohorT?usp=sharing). Save the downloaded `fasta` folder inside the `inphared` directory located in the same folder as this notebook. The folder structure should look like this:\n",
    "\n",
    "`experiments` (parent folder of this notebook) <br> \n",
    "‚Ü≥ `inphared` <br>\n",
    "&nbsp; &nbsp;‚Ü≥ `fasta` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `hypothetical` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `nucleotide` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `rbp` <br>\n",
    "‚Ü≥ `4. Protein Embedding Generation.ipynb` (this notebook) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Output Files\n",
    "If you would like to skip running this notebook, you may download the protein embeddings from these Google Drive directories: [Part 1](https://drive.google.com/drive/folders/1deenrDQIr3xcl9QCYH-nPhmpY8x2drQw?usp=sharing) and [Part 2](https://drive.google.com/drive/folders/1jnBFNsC6zJISkc6IAz56257MSXKjY0Ez?usp=sharing). Consolidate the downloaded folders into a single `embeddings` directory and save it inside the `inphared` directory located in the same folder as this notebook. The folder structure should look like this:\n",
    "\n",
    "`experiments` (parent folder of this notebook) <br> \n",
    "‚Ü≥ `inphared` <br>\n",
    "&nbsp; &nbsp;‚Ü≥ `embeddings` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `esm` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ `esm1b` <br>\n",
    "&nbsp; &nbsp;&nbsp; &nbsp; ‚Ü≥ ... <br>\n",
    "‚Ü≥ `4. Protein Embedding Generation.ipynb` (this notebook) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below if you are using a cloud-based service."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLgiuc5oZcNu",
    "outputId": "37b73e1b-8b7b-4b5b-8747-1c230412a70d"
   },
   "source": [
    "!pip3 install -U pip > /dev/null\n",
    "!pip3 install -U bio_embeddings[all] > /dev/null\n",
    "!pip install scikit_learn==1.0.2\n",
    "!pip install pyyaml==5.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi4yylSXZf6O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the protein language model. \n",
    "\n",
    "Protein Language Model | Import\n",
    "-- | --\n",
    "SeqVec | `SeqVecEmbedder`\n",
    "ESM | `ESMEmbedder`\n",
    "ESM-1b | `ESM1bEmbedder`\n",
    "ProtBert | `ProtTransBertBFDEmbedder`\n",
    "ProtXLNet | `ProtTransXLNetUniRef100Embedder`\n",
    "ProtAlbert | `ProtTransAlbertBFDEmbedder`\n",
    "ProtT5 | `ProtTransT5XLU50Embedder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkM73XzOB7MP"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Generation of Protein Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below generate the protein embeddings for the proteins in a given FASTA file:\n",
    "- Use `compute_protein_embeddings_esm` for ESM and ESM-1b. Sequences longer than 1022 amino acids are split into non-overlapping subsequences of length 1022, and the per-residue embeddings are concatenated before averaging (this is the [workaround](https://github.com/brianhie/evolocity/issues/2) suggested by the developers).\n",
    "- Use `compute_protein_embeddings` for all other language models.\n",
    "\n",
    "**Parameters**:\n",
    "- `embedder`: Protein language model\n",
    "- `fasta_file`: FASTA file containing the proteins\n",
    "- `results_dir`: File path of the directory to which the resulting embeddings will be saved\n",
    "- `prefix`: Name of the phage whose selected proteins are to be converted to be embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S43Mg3StZh4N"
   },
   "outputs": [],
   "source": [
    "def compute_protein_embeddings(embedder, fasta_file, results_dir, prefix=\"\"):\n",
    "    names = [record.id for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "    sequences = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "    embeddings = [\n",
    "        embedder.reduce_per_protein(embedder.embed(sequence))\n",
    "        for sequence in tqdm(sequences)\n",
    "    ]\n",
    "    embeddings_df = pd.concat(\n",
    "        [pd.DataFrame({\"ID\": names}), pd.DataFrame(embeddings)], axis=1\n",
    "    )\n",
    "    embeddings_df.to_csv(results_dir + prefix + \"-embeddings.csv\", index=False)\n",
    "\n",
    "\n",
    "def compute_protein_embeddings_esm(embedder, fasta_file, results_dir, prefix=\"\"):\n",
    "    names = [record.id for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        if len(sequence) <= 1022:\n",
    "            embedding = embedder.reduce_per_protein(embedder.embed(sequence))\n",
    "        else:\n",
    "            embedding1 = embedder.embed(sequence[:1022])\n",
    "            embedding2 = embedder.embed(sequence[1022:])\n",
    "            embedding = embedder.reduce_per_protein(\n",
    "                np.concatenate((embedding1, embedding2))\n",
    "            )\n",
    "\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    embeddings_df = pd.concat(\n",
    "        [pd.DataFrame({\"ID\": names}), pd.DataFrame(embeddings)], axis=1\n",
    "    )\n",
    "    embeddings_df.to_csv(results_dir + prefix + \"-embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the protein language model.\n",
    "\n",
    "Protein Language Model | Constructor\n",
    "-- | --\n",
    "SeqVec | `SeqVecEmbedder`\n",
    "ESM | `ESMEmbedder`\n",
    "ESM-1b | `ESM1bEmbedder`\n",
    "ProtBert | `ProtTransBertBFDEmbedder`\n",
    "ProtXLNet | `ProtTransXLNetUniRef100Embedder`\n",
    "ProtAlbert | `ProtTransAlbertBFDEmbedder`\n",
    "ProtT5 | `ProtTransT5XLU50Embedder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfPGFC17bedX"
   },
   "outputs": [],
   "source": [
    "embedder = ProtTransBertBFDEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply the directory names:\n",
    "- `FASTA_DIR`: Directory where the FASTA files containing the protein sequences are located\n",
    "- `EMBEDDINGS_DIR`: Directory where the CSV files containing the embeddings are to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWdkkpilF-gF"
   },
   "outputs": [],
   "source": [
    "FASTA_DIR = f\"\"\n",
    "EMBEDDINGS_DIR = f\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the FASTA files containing the protein sequences to be embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLNKoLIgF1l2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fasta_files = os.listdir(FASTA_DIR)\n",
    "\n",
    "print(len(fasta_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the protein embeddings.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: If the embedder is ESM or ESM-1b, call `compute_protein_embeddings_esm` instead of `compute_protein_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNbJ4LawFg9_"
   },
   "outputs": [],
   "source": [
    "IDX_RESUME = (\n",
    "    0  # Adjust as needed (e.g., resuming after Google Colab hangs or times out)\n",
    ")\n",
    "\n",
    "for file in fasta_files[IDX_RESUME:]:\n",
    "    # -6 because the string \".fasta\" has six characters\n",
    "    compute_protein_embeddings(\n",
    "        embedder, f\"{FASTA_DIR}/{file}\", EMBEDDINGS_DIR, f\"/{file[:-6]}\"\n",
    "    )\n",
    "\n",
    "    # Display progress\n",
    "    print(IDX_RESUME, \":\", file)\n",
    "    IDX_RESUME += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
